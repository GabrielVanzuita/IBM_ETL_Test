# ğŸŒ ETL Project â€“ Country GDP Data, but With Style

## ğŸ§  Overview
This project is a full ETL (Extract, Transform, Load) pipeline built to turn raw Country GDP data into clean, structured insights.

We extract a table from the web, enhance it with real-world exchange rates, and load the final product into both a CSV file and a local SQLite database â€” all while keeping detailed logs of every step.

Because data science without traceability is just expensive guessing.

---

## ğŸ—‚ï¸ Project Structure
```
project_root/
â”‚
â”œâ”€â”€ data/                     # Raw and processed data lives here
â”‚   â”œâ”€â”€ exchange_rates.csv    # Daily exchange rates
â”‚   â””â”€â”€ output/               # Output directory for transformed results
â”‚       â””â”€â”€ country_gdp_transformed.csv
â”‚
â”œâ”€â”€ logs/                     # ETL process logs
â”‚   â”œâ”€â”€ etl_progress.log
â”‚   â”œâ”€â”€ test_extract.log
â”‚   â”œâ”€â”€ test_transform.log
â”‚   â””â”€â”€ test_db_operations.log
â”‚
â”œâ”€â”€ src/                      # Core ETL logic
â”‚   â”œâ”€â”€ extractor.py          # Extracts data from the web
â”‚   â”œâ”€â”€ transformer.py        # Handles currency conversion & transformation
â”‚   â”œâ”€â”€ loader.py             # Loads data into CSV and SQLite
â”‚   â””â”€â”€ main.py               # Orchestrates the ETL pipeline
â”‚
â”œâ”€â”€ tests/                    # Test suite
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ .gitignore                # Files to ignore in version control
â”œâ”€â”€ README.md                 # You're reading it
â””â”€â”€ requirements.txt          # Project dependencies
```

---

## ğŸ“¦ Requirements
- Python 3.x
- Packages used:
  - `requests`
  - `pandas`
  - `beautifulsoup4`
  - `lxml`
  - `html5lib`

Install them all with:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Use

1. Make sure `exchange_rates.csv` is present in the `data/` directory.
2. Run the main script to launch the ETL process:

```bash
python src/main.py
```

The output will be stored in `data/output/`, and a SQLite database will be updated or created alongside it.

---

## ğŸ“ Logging

Every action the pipeline takes â€” from extraction to transformation to loading â€” is logged with timestamps. Youâ€™ll find those logs in the `logs/` folder, neatly organized by stage.

---

## âš–ï¸ License

This project is licensed under the MIT License.  
Feel free to use, modify, break, or build upon it â€” just don't forget to give credit where credit is due.